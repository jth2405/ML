{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04a4fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info > (3, 5)\n",
    "\n",
    "import sklearn \n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26881fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test/ 255.0\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "model = keras.models.Sequential([ \n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\") \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0ee70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) \n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 \n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) \n",
    "    \n",
    "    return ((X[~y_5_or_6], y_A), (X[y_5_or_6], y_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27b34f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train_A.shape:  (43986, 28, 28)\n",
      "\n",
      "X_train_B.shape:  (200, 28, 28)\n",
      "\n",
      "y_train_A[:30]:  [4 0 5 7 7 7 4 4 3 4 0 1 6 3 4 3 2 6 5 3 4 5 1 3 4 2 0 6 7 1]\n",
      "\n",
      "y_train_B[:30]: [1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 1. 0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]\n",
    "#2017250045 정태환\n",
    "print(\"\\nX_train_A.shape: \", X_train_A.shape)\n",
    "print(\"\\nX_train_B.shape: \", X_train_B.shape)\n",
    "print(\"\\ny_train_A[:30]: \", y_train_A[:30])\n",
    "print(\"\\ny_train_B[:30]:\", y_train_B[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de0ae108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.5701 - accuracy: 0.8172 - val_loss: 0.3740 - val_accuracy: 0.8772\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.3507 - accuracy: 0.8792 - val_loss: 0.3171 - val_accuracy: 0.8939\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.3150 - accuracy: 0.8902 - val_loss: 0.3010 - val_accuracy: 0.8976\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2962 - accuracy: 0.8975 - val_loss: 0.2809 - val_accuracy: 0.9108\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2831 - accuracy: 0.9026 - val_loss: 0.2753 - val_accuracy: 0.9106\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2734 - accuracy: 0.9061 - val_loss: 0.2636 - val_accuracy: 0.9160\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2656 - accuracy: 0.9099 - val_loss: 0.2602 - val_accuracy: 0.9158\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2582 - accuracy: 0.9130 - val_loss: 0.2558 - val_accuracy: 0.9168\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2526 - accuracy: 0.9144 - val_loss: 0.2533 - val_accuracy: 0.9150\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2474 - accuracy: 0.9160 - val_loss: 0.2461 - val_accuracy: 0.9195\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2423 - accuracy: 0.9176 - val_loss: 0.2462 - val_accuracy: 0.9165\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2384 - accuracy: 0.9175 - val_loss: 0.2447 - val_accuracy: 0.9153\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2348 - accuracy: 0.9211 - val_loss: 0.2394 - val_accuracy: 0.9195\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2312 - accuracy: 0.9207 - val_loss: 0.2608 - val_accuracy: 0.9046\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2283 - accuracy: 0.9220 - val_loss: 0.2404 - val_accuracy: 0.9180\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2256 - accuracy: 0.9228 - val_loss: 0.2352 - val_accuracy: 0.9188\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2223 - accuracy: 0.9241 - val_loss: 0.2318 - val_accuracy: 0.9203\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2193 - accuracy: 0.9251 - val_loss: 0.2308 - val_accuracy: 0.9205\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2168 - accuracy: 0.9253 - val_loss: 0.2329 - val_accuracy: 0.9213\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2147 - accuracy: 0.9261 - val_loss: 0.2320 - val_accuracy: 0.9203\n"
     ]
    }
   ],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
    "\n",
    "model_A.compile(loss= \"sparse_categorical_crossentropy\",\n",
    "    optimizer= keras.optimizers.SGD(learning_rate=1e-3),\n",
    "    metrics=[\"accuracy\"])\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20, \n",
    "                      validation_data=(X_valid_A, y_valid_A))\n",
    "model_A.save(\"my_model_A.h5\")\n",
    "#2017250045 정태환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ca84377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 31ms/step - loss: 0.4615 - accuracy: 0.7700 - val_loss: 0.4093 - val_accuracy: 0.8164\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.3297 - accuracy: 0.9000 - val_loss: 0.3179 - val_accuracy: 0.9128\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2546 - accuracy: 0.9600 - val_loss: 0.2628 - val_accuracy: 0.9412\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2076 - accuracy: 0.9700 - val_loss: 0.2226 - val_accuracy: 0.9564\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1731 - accuracy: 0.9900 - val_loss: 0.1957 - val_accuracy: 0.9594\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.1492 - accuracy: 0.9900 - val_loss: 0.1763 - val_accuracy: 0.9635\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1315 - accuracy: 0.9900 - val_loss: 0.1619 - val_accuracy: 0.9675\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1185 - accuracy: 0.9950 - val_loss: 0.1497 - val_accuracy: 0.9706\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1072 - accuracy: 0.9950 - val_loss: 0.1391 - val_accuracy: 0.9746\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0982 - accuracy: 0.9950 - val_loss: 0.1308 - val_accuracy: 0.9767\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0907 - accuracy: 0.9950 - val_loss: 0.1227 - val_accuracy: 0.9787\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0836 - accuracy: 0.9950 - val_loss: 0.1163 - val_accuracy: 0.9787\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0782 - accuracy: 0.9950 - val_loss: 0.1110 - val_accuracy: 0.9787\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0732 - accuracy: 0.9950 - val_loss: 0.1062 - val_accuracy: 0.9787\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0690 - accuracy: 0.9950 - val_loss: 0.1023 - val_accuracy: 0.9807\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0655 - accuracy: 0.9950 - val_loss: 0.0984 - val_accuracy: 0.9807\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0622 - accuracy: 0.9950 - val_loss: 0.0951 - val_accuracy: 0.9828\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0592 - accuracy: 0.9950 - val_loss: 0.0916 - val_accuracy: 0.9838\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0562 - accuracy: 0.9950 - val_loss: 0.0888 - val_accuracy: 0.9848\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0537 - accuracy: 0.9950 - val_loss: 0.0863 - val_accuracy: 0.9848\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235200    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30000     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_B.compile(loss= \"binary_crossentropy\",\n",
    "    optimizer= keras.optimizers.SGD(learning_rate=1e-3),\n",
    "    metrics=[\"accuracy\"])\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20, \n",
    "                      validation_data=(X_valid_B, y_valid_B))\n",
    "#2017250045 정태환\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9f5f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 1s 31ms/step - loss: 1.9690 - accuracy: 0.0900 - val_loss: 1.7444 - val_accuracy: 0.1511\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.8124 - accuracy: 0.1300 - val_loss: 1.6101 - val_accuracy: 0.1866\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.6690 - accuracy: 0.1550 - val_loss: 1.4764 - val_accuracy: 0.2211\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.5272 - accuracy: 0.1850 - val_loss: 1.3607 - val_accuracy: 0.2606\n"
     ]
    }
   ],
   "source": [
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1]) \n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_A_clone = keras.models.clone_model(model_A) \n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]: \n",
    "    layer.trainable = False\n",
    "#2017250045 정태환\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", \n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93fe8631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 33ms/step - loss: 1.1837 - accuracy: 0.3400 - val_loss: 0.8121 - val_accuracy: 0.5548\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.7177 - accuracy: 0.6300 - val_loss: 0.5277 - val_accuracy: 0.7404\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.4665 - accuracy: 0.7800 - val_loss: 0.3827 - val_accuracy: 0.8489\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.3359 - accuracy: 0.8650 - val_loss: 0.3016 - val_accuracy: 0.9057\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2616 - accuracy: 0.9100 - val_loss: 0.2496 - val_accuracy: 0.9320\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2129 - accuracy: 0.9550 - val_loss: 0.2121 - val_accuracy: 0.9503\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.1780 - accuracy: 0.9750 - val_loss: 0.1851 - val_accuracy: 0.9625\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1523 - accuracy: 0.9850 - val_loss: 0.1651 - val_accuracy: 0.9655\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1335 - accuracy: 0.9850 - val_loss: 0.1489 - val_accuracy: 0.9726\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1181 - accuracy: 0.9900 - val_loss: 0.1367 - val_accuracy: 0.9757\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1067 - accuracy: 0.9900 - val_loss: 0.1267 - val_accuracy: 0.9787\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0973 - accuracy: 0.9950 - val_loss: 0.1181 - val_accuracy: 0.9817\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0892 - accuracy: 0.9950 - val_loss: 0.1099 - val_accuracy: 0.9817\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0816 - accuracy: 0.9950 - val_loss: 0.1029 - val_accuracy: 0.9828\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0752 - accuracy: 0.9950 - val_loss: 0.0978 - val_accuracy: 0.9848\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0704 - accuracy: 0.9950 - val_loss: 0.0933 - val_accuracy: 0.9848\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True \n",
    "#2017250045 정태환    \n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"]) \n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85aedefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5411 - accuracy: 0.8127 - val_loss: 0.3871 - val_accuracy: 0.8642\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3932 - accuracy: 0.8601 - val_loss: 0.3518 - val_accuracy: 0.8756\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3546 - accuracy: 0.8723 - val_loss: 0.3274 - val_accuracy: 0.8874\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3265 - accuracy: 0.8820 - val_loss: 0.3230 - val_accuracy: 0.8890\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3080 - accuracy: 0.8879 - val_loss: 0.3236 - val_accuracy: 0.8830\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2923 - accuracy: 0.8937 - val_loss: 0.3102 - val_accuracy: 0.8880\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2778 - accuracy: 0.8999 - val_loss: 0.3104 - val_accuracy: 0.8904\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2640 - accuracy: 0.9054 - val_loss: 0.3069 - val_accuracy: 0.8884\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2527 - accuracy: 0.9075 - val_loss: 0.3056 - val_accuracy: 0.8928\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2459 - accuracy: 0.9110 - val_loss: 0.3038 - val_accuracy: 0.8890\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235200    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30000     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9),\n",
    "              metrics=[\"accuracy\"])\n",
    "#2017250045 정태환\n",
    "history = model.fit(X_train, y_train, epochs=10, \n",
    "                      validation_data=(X_valid, y_valid))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b787fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2331 - accuracy: 0.9140 - val_loss: 0.3043 - val_accuracy: 0.8906\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2228 - accuracy: 0.9175 - val_loss: 0.3032 - val_accuracy: 0.8906\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2165 - accuracy: 0.9202 - val_loss: 0.3053 - val_accuracy: 0.8920\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2090 - accuracy: 0.9229 - val_loss: 0.3076 - val_accuracy: 0.8922\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2005 - accuracy: 0.9271 - val_loss: 0.3130 - val_accuracy: 0.8906\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1921 - accuracy: 0.9305 - val_loss: 0.3144 - val_accuracy: 0.8884\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1887 - accuracy: 0.9308 - val_loss: 0.3117 - val_accuracy: 0.8934\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1801 - accuracy: 0.9341 - val_loss: 0.3070 - val_accuracy: 0.8942\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1750 - accuracy: 0.9363 - val_loss: 0.3300 - val_accuracy: 0.8894\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1701 - accuracy: 0.9374 - val_loss: 0.3201 - val_accuracy: 0.8924\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235200    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30000     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True),\n",
    "              metrics=[\"accuracy\"])\n",
    "#2017250045 정태환\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a06ae8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adagrad.py:74: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adagrad, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.1416 - accuracy: 0.9492 - val_loss: 0.3053 - val_accuracy: 0.8952\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1340 - accuracy: 0.9525 - val_loss: 0.3027 - val_accuracy: 0.8982\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1318 - accuracy: 0.9537 - val_loss: 0.3098 - val_accuracy: 0.8958\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1262 - accuracy: 0.9571 - val_loss: 0.3082 - val_accuracy: 0.8938\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1277 - accuracy: 0.9549 - val_loss: 0.3095 - val_accuracy: 0.8946\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1238 - accuracy: 0.9573 - val_loss: 0.3122 - val_accuracy: 0.8954\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.1223 - accuracy: 0.9569 - val_loss: 0.3114 - val_accuracy: 0.8966\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1221 - accuracy: 0.9570 - val_loss: 0.3138 - val_accuracy: 0.8962\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1218 - accuracy: 0.9571 - val_loss: 0.3106 - val_accuracy: 0.8960\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1205 - accuracy: 0.9581 - val_loss: 0.3135 - val_accuracy: 0.8954\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235200    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30000     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer = keras.optimizers.Adagrad(lr=0.001),\n",
    "              metrics=[\"accuracy\"])\n",
    "#2017250045 정태환\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid)) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e274b8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4149 - accuracy: 0.8512 - val_loss: 0.3731 - val_accuracy: 0.8724\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3384 - accuracy: 0.8771 - val_loss: 0.3413 - val_accuracy: 0.8834\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3071 - accuracy: 0.8890 - val_loss: 0.3201 - val_accuracy: 0.8876\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2851 - accuracy: 0.8954 - val_loss: 0.3269 - val_accuracy: 0.8862\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2716 - accuracy: 0.9012 - val_loss: 0.3226 - val_accuracy: 0.8884\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.2560 - accuracy: 0.9066 - val_loss: 0.3345 - val_accuracy: 0.8890\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.2431 - accuracy: 0.9121 - val_loss: 0.3162 - val_accuracy: 0.8948\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2328 - accuracy: 0.9145 - val_loss: 0.3096 - val_accuracy: 0.8952\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2254 - accuracy: 0.9170 - val_loss: 0.3031 - val_accuracy: 0.8986\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.2165 - accuracy: 0.9210 - val_loss: 0.3272 - val_accuracy: 0.8954\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235200    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30000     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9), metrics=[\"accuracy\"])\n",
    "#2017250045 정태환\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid)) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "455d7efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2127 - accuracy: 0.9211 - val_loss: 0.3299 - val_accuracy: 0.8912\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2022 - accuracy: 0.9263 - val_loss: 0.3136 - val_accuracy: 0.8984\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1934 - accuracy: 0.9278 - val_loss: 0.3174 - val_accuracy: 0.8950\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.1848 - accuracy: 0.9307 - val_loss: 0.3058 - val_accuracy: 0.9024\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.1769 - accuracy: 0.9348 - val_loss: 0.3280 - val_accuracy: 0.8938\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1695 - accuracy: 0.9369 - val_loss: 0.3451 - val_accuracy: 0.8956\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.1642 - accuracy: 0.9385 - val_loss: 0.3213 - val_accuracy: 0.8974\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.1570 - accuracy: 0.9415 - val_loss: 0.3500 - val_accuracy: 0.8956\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.1530 - accuracy: 0.9427 - val_loss: 0.3412 - val_accuracy: 0.8990\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.1483 - accuracy: 0.9442 - val_loss: 0.3637 - val_accuracy: 0.8916\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235200    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30000     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=[\"accuracy\"])\n",
    "#2017250045 정태환\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid)) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f41b594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adamax.py:90: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adamax, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.1041 - accuracy: 0.9621 - val_loss: 0.3314 - val_accuracy: 0.9024\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0925 - accuracy: 0.9668 - val_loss: 0.3459 - val_accuracy: 0.9020\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0874 - accuracy: 0.9690 - val_loss: 0.3514 - val_accuracy: 0.9082\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.0834 - accuracy: 0.9693 - val_loss: 0.3543 - val_accuracy: 0.9038\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.0810 - accuracy: 0.9705 - val_loss: 0.3649 - val_accuracy: 0.9044\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0777 - accuracy: 0.9711 - val_loss: 0.3689 - val_accuracy: 0.9062\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.0748 - accuracy: 0.9735 - val_loss: 0.3736 - val_accuracy: 0.9080\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0743 - accuracy: 0.9731 - val_loss: 0.3764 - val_accuracy: 0.9048\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.0719 - accuracy: 0.9731 - val_loss: 0.3862 - val_accuracy: 0.9036\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0702 - accuracy: 0.9742 - val_loss: 0.3836 - val_accuracy: 0.9058\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235200    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30000     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer = keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999), \n",
    "              metrics=[\"accuracy\"])\n",
    "#2017250045 정태환\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid)) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "618b217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\nadam.py:73: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Nadam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1230 - accuracy: 0.9541 - val_loss: 0.3906 - val_accuracy: 0.8918\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1225 - accuracy: 0.9547 - val_loss: 0.3863 - val_accuracy: 0.8926\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1210 - accuracy: 0.9531 - val_loss: 0.3800 - val_accuracy: 0.8970\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1209 - accuracy: 0.9546 - val_loss: 0.3641 - val_accuracy: 0.9018\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1184 - accuracy: 0.9555 - val_loss: 0.3800 - val_accuracy: 0.8992\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1137 - accuracy: 0.9576 - val_loss: 0.3928 - val_accuracy: 0.8942\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1099 - accuracy: 0.9589 - val_loss: 0.4009 - val_accuracy: 0.8960\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1098 - accuracy: 0.9585 - val_loss: 0.3871 - val_accuracy: 0.8986\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1062 - accuracy: 0.9601 - val_loss: 0.3812 - val_accuracy: 0.8978\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1006 - accuracy: 0.9620 - val_loss: 0.3993 - val_accuracy: 0.8990\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235200    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30000     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999), \n",
    "              metrics=[\"accuracy\"])\n",
    "#2017250045 정태환\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid)) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "775d88ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4913 - accuracy: 0.8283 - val_loss: 0.4127 - val_accuracy: 0.8608\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3817 - accuracy: 0.8647 - val_loss: 0.3858 - val_accuracy: 0.8678\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3490 - accuracy: 0.8762 - val_loss: 0.3649 - val_accuracy: 0.8748\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3264 - accuracy: 0.8842 - val_loss: 0.3543 - val_accuracy: 0.8792\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3107 - accuracy: 0.8894 - val_loss: 0.3457 - val_accuracy: 0.8820\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2979 - accuracy: 0.8945 - val_loss: 0.3451 - val_accuracy: 0.8802\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2868 - accuracy: 0.8975 - val_loss: 0.3458 - val_accuracy: 0.8818\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2778 - accuracy: 0.9020 - val_loss: 0.3379 - val_accuracy: 0.8852\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2692 - accuracy: 0.9046 - val_loss: 0.3321 - val_accuracy: 0.8846\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2626 - accuracy: 0.9064 - val_loss: 0.3370 - val_accuracy: 0.8804\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2560 - accuracy: 0.9095 - val_loss: 0.3312 - val_accuracy: 0.8834\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2504 - accuracy: 0.9113 - val_loss: 0.3299 - val_accuracy: 0.8862\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2450 - accuracy: 0.9139 - val_loss: 0.3298 - val_accuracy: 0.8846\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2406 - accuracy: 0.9147 - val_loss: 0.3290 - val_accuracy: 0.8858\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2358 - accuracy: 0.9173 - val_loss: 0.3220 - val_accuracy: 0.8856\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2320 - accuracy: 0.9190 - val_loss: 0.3210 - val_accuracy: 0.8878\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2283 - accuracy: 0.9197 - val_loss: 0.3229 - val_accuracy: 0.8872\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2245 - accuracy: 0.9214 - val_loss: 0.3231 - val_accuracy: 0.8894\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2209 - accuracy: 0.9230 - val_loss: 0.3253 - val_accuracy: 0.8842\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2181 - accuracy: 0.9247 - val_loss: 0.3194 - val_accuracy: 0.8872\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2148 - accuracy: 0.9261 - val_loss: 0.3180 - val_accuracy: 0.8904\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2117 - accuracy: 0.9276 - val_loss: 0.3211 - val_accuracy: 0.8888\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2090 - accuracy: 0.9271 - val_loss: 0.3183 - val_accuracy: 0.8896\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2068 - accuracy: 0.9289 - val_loss: 0.3193 - val_accuracy: 0.8860\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2040 - accuracy: 0.9291 - val_loss: 0.3208 - val_accuracy: 0.8854\n"
     ]
    }
   ],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\") ])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "#2017250045 정태환\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5257ad4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "decay = 1e-4 \n",
    "batch_size = 32\n",
    "n_steps_per_epoch = len(X_train) // batch_size\n",
    "epochs = np.arange(n_epochs)\n",
    "lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)\n",
    "\n",
    "plt.plot(epochs, lrs, \"o-\") \n",
    "plt.axis([0, n_epochs - 1, 0, 0.01])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\") \n",
    "plt.title(\"Power Scheduling\", fontsize=14) \n",
    "plt.grid(True)\n",
    "#2017250045 정태환\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "130a9769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.5698 - accuracy: 0.8008 - val_loss: 0.3763 - val_accuracy: 0.8662\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4212 - accuracy: 0.8452 - val_loss: 0.3495 - val_accuracy: 0.8696\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_9 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 300)               0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"]) \n",
    "n_epochs = 2\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid))\n",
    "#2017250045 정태환\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cfd3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
